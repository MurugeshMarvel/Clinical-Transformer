{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from args import define_main_parser\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "#from dataset.prsa import PRSADataset\n",
    "from data.card import TransactionDataset\n",
    "from models.modules import TabFormerBertLM\n",
    "from scripts.utils import random_split_dataset\n",
    "from data.datacollator import TransDataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "config = vars(Namespace(cached=False, checkpoint=0, data_extension='', data_fname='card_transaction.v1', data_root='./data/credit_card/', data_type='card', do_eval=False, do_train=True, field_ce=True, field_hs=64, flatten=False, jid=1, lm_type='bert', log_dir='sam/logs', mlm=True, mlm_prob=0.15, nrows=None, num_train_epochs=3, output_dir='sam', save_steps=500, seed=9, skip_user=False, stride=5, user_ids=None, vocab_file='vocab.nb'))\n",
    "config['data_root'] = \"../dataset/credit_card/\"\n",
    "config['output_dir'] = \"sample\"\n",
    "config['log_dir'] = \"sample/logs\"\n",
    "makedirs(config['output_dir'], exist_ok=True)\n",
    "makedirs(config['log_dir'], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = config['seed']\n",
    "random.seed(seed)  # python\n",
    "np.random.seed(seed)  # numpy\n",
    "torch.manual_seed(seed)  # torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)  # torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransactionDataset(root=config['data_root'],\n",
    "                            fname=config['data_fname'],\n",
    "                            fextension=\"\",\n",
    "                            vocab_dir=config['output_dir'],\n",
    "                            nrows=None,\n",
    "                            user_ids=None,\n",
    "                            seq_len=20,\n",
    "                            mlm=config['mlm'],\n",
    "                            cached=config['cached'],\n",
    "                            stride=10,\n",
    "                            flatten=config['flatten'],\n",
    "                            return_labels=False,\n",
    "                            skip_user=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.vocab\n",
    "custom_special_tokens = vocab.get_special_tokens()\n",
    "\n",
    "totalN = len(dataset)\n",
    "totalN = len(dataset)\n",
    "trainN = int(0.6 * totalN)\n",
    "\n",
    "valtestN = totalN - trainN\n",
    "valN = int(valtestN * 0.5)\n",
    "testN = valtestN - valN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [trainN, valN, testN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# lengths: train [599]  valid [200]  test [200]\n",
      "# lengths: train [0.60]  valid [0.20]  test [0.20]\n"
     ]
    }
   ],
   "source": [
    "print(f\"# lengths: train [{trainN}]  valid [{valN}]  test [{testN}]\")\n",
    "print(\"# lengths: train [{:.2f}]  valid [{:.2f}]  test [{:.2f}]\".format(trainN / totalN, valN / totalN,\n",
    "                                                                               testN / totalN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = random_split_dataset(dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.dataset.__getitem__(1899)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                                  vocab=vocab,\n",
    "                                  field_ce=config['field_ce'],\n",
    "                                  flatten=config['flatten'],\n",
    "                                  ncols=dataset.ncols,\n",
    "                                  field_hidden_size=config['field_hs']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collactor_cls = \"TransDataCollatorForLanguageModeling\"\n",
    "data_collator = eval(collactor_cls)(\n",
    "        tokenizer=tab_net.tokenizer, mlm=config['mlm'], mlm_probability=config['mlm_prob']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=config['output_dir'],  # output directory\n",
    "        num_train_epochs=config['num_train_epochs'],  # total number of training epochs\n",
    "        logging_dir=config['log_dir'],  # directory for storing logs\n",
    "        save_steps=config['save_steps'],\n",
    "        do_train=config['do_train'],\n",
    "        # do_eval=args.do_eval,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        prediction_loss_only=True,\n",
    "        overwrite_output_dir=True,\n",
    "        # eval_steps=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=tab_net.model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=10,\n",
    "            collate_fn=data_collator        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tab_net.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'masked_lm_labels'])\n",
      "torch.Size([10, 20, 11])\n",
      "torch.Size([10, 20, 11])\n",
      "Last Hidden State shape torch.Size([10, 20, 704])\n",
      "Sequence Output shape - torch.Size([10, 20, 704])\n",
      "Output shape - [10, 20, 704]\n",
      "Expected shape - [10, 220, -1]\n",
      "Sequence output - torch.Size([10, 220, 64])\n",
      "Masked lm labels - torch.Size([10, 220])\n",
      "Prediction score shape - torch.Size([10, 220, 1062])\n",
      "[0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 121, 132, 143, 154, 165, 176, 187, 198, 209] for Card\n",
      "Prediction score for loss - torch.Size([200, 3])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 122, 133, 144, 155, 166, 177, 188, 199, 210] for Timestamp\n",
      "Prediction score for loss - torch.Size([200, 10])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[2, 13, 24, 35, 46, 57, 68, 79, 90, 101, 112, 123, 134, 145, 156, 167, 178, 189, 200, 211] for Amount\n",
      "Prediction score for loss - torch.Size([200, 10])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[3, 14, 25, 36, 47, 58, 69, 80, 91, 102, 113, 124, 135, 146, 157, 168, 179, 190, 201, 212] for Use Chip\n",
      "Prediction score for loss - torch.Size([200, 3])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[4, 15, 26, 37, 48, 59, 70, 81, 92, 103, 114, 125, 136, 147, 158, 169, 180, 191, 202, 213] for Merchant Name\n",
      "Prediction score for loss - torch.Size([200, 407])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[5, 16, 27, 38, 49, 60, 71, 82, 93, 104, 115, 126, 137, 148, 159, 170, 181, 192, 203, 214] for Merchant City\n",
      "Prediction score for loss - torch.Size([200, 209])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[6, 17, 28, 39, 50, 61, 72, 83, 94, 105, 116, 127, 138, 149, 160, 171, 182, 193, 204, 215] for Merchant State\n",
      "Prediction score for loss - torch.Size([200, 38])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[7, 18, 29, 40, 51, 62, 73, 84, 95, 106, 117, 128, 139, 150, 161, 172, 183, 194, 205, 216] for Zip\n",
      "Prediction score for loss - torch.Size([200, 284])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[8, 19, 30, 41, 52, 63, 74, 85, 96, 107, 118, 129, 140, 151, 162, 173, 184, 195, 206, 217] for MCC\n",
      "Prediction score for loss - torch.Size([200, 81])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[9, 20, 31, 42, 53, 64, 75, 86, 97, 108, 119, 130, 141, 152, 163, 174, 185, 196, 207, 218] for Errors?\n",
      "Prediction score for loss - torch.Size([200, 8])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 131, 142, 153, 164, 175, 186, 197, 208, 219] for SPECIAL\n",
      "Prediction score for loss - torch.Size([200, 7])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "2\n",
      "Length of out -  1\n",
      "Ouput - torch.Size([10, 220, 1062])\n",
      "tensor(nan, grad_fn=<AddBackward0>)\n",
      "torch.Size([10, 220, 1062])\n"
     ]
    }
   ],
   "source": [
    "for inps in train_dataloader:\n",
    "    print(inps.keys())\n",
    "    print(inps['input_ids'].shape)\n",
    "    print(inps['masked_lm_labels'].shape)\n",
    "    #print(inps['masked_lm_labels'], )\n",
    "    aa, out =model(**inps)\n",
    "    print(len(aa))\n",
    "    print(\"Length of out - \", len(out))\n",
    "    print('Ouput -', out[0].shape)\n",
    "    #print('Ouput -', out[1].shape)\n",
    "    print(aa[0])\n",
    "    print(aa[1].shape)\n",
    "    #aa.last_hidden_state\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in new_mode/config.json\n",
      "Model weights saved in new_mode/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "tab_net.model.save_pretrained('new_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                                  vocab=vocab,\n",
    "                                  field_ce=config['field_ce'],\n",
    "                                  flatten=config['flatten'],\n",
    "                                  ncols=dataset.ncols,\n",
    "                                  field_hidden_size=config['field_hs']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_net.model.tb_model.save_pretrained('new_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_net.model.tb_model = tab_net.model.tb_model.from_pretrained('new_mode',vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_net.model.tab_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_net.model.tb_model = tab_net.model.tb_model.from_pretrained('new_mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(tab_net.model.tb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tab_net.model.tb_model.bert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_classifier import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_fe_model = tab_net.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = LSTM(emb_inp_size=1062)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                                  vocab=vocab,\n",
    "                                  field_ce=config['field_ce'],\n",
    "                                  flatten=config['flatten'],\n",
    "                                  ncols=dataset.ncols,\n",
    "                                  field_hidden_size=config['field_hs']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, custom_special_tokens,\n",
    "                 vocab,\n",
    "                 field_ce,\n",
    "                 flatten,\n",
    "                 ncols,\n",
    "                 field_hidden_size,\n",
    "                 bert_feature_size\n",
    "                 ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                                        vocab=vocab,\n",
    "                                        field_ce=field_ce,\n",
    "                                        flatten=flatten,\n",
    "                                        ncols=ncols,\n",
    "                                        field_hidden_size=field_hidden_size)\n",
    "\n",
    "        self.field_transformer = self.tab_net.model.tab_embeddings\n",
    "        self.bert = self.tab_net.model.tb_model\n",
    "\n",
    "\n",
    "        self.classifier = LSTM(emb_inp_size=bert_feature_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids ,input_args):\n",
    "        field_embeddings = self.field_transformer(input_ids)\n",
    "        #input_args['input_ids'] = input_ids\n",
    "        bert_features = self.bert(inputs_embeds=field_embeddings, **input_args)\n",
    "        \n",
    "        bert_features = bert_features[1]\n",
    "        bert_features = bert_features.reshape((10, 20, 11, 1062))\n",
    "        bert_features = bert_features.reshape((200, 11, 1062))\n",
    "        cls_out = self.classifier(bert_features, T.as_tensor(([11])))\n",
    "        return cls_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = Classifier(custom_special_tokens,\n",
    "                 vocab=vocab,\n",
    "                    field_ce=config['field_ce'],\n",
    "                    flatten=config['flatten'],\n",
    "                    ncols=dataset.ncols,\n",
    "                    field_hidden_size=config['field_hs'],\n",
    "                 bert_feature_size=1062)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([232])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as T\n",
    "T.as_tensor(([232]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 11])\n",
      "torch.Size([10, 20, 11])\n",
      "dict_keys(['masked_lm_labels'])\n",
      "Last Hidden State shape torch.Size([10, 20, 704])\n",
      "Sequence Output shape - torch.Size([10, 20, 704])\n",
      "Output shape - [10, 20, 704]\n",
      "Expected shape - [10, 220, -1]\n",
      "Sequence output - torch.Size([10, 220, 64])\n",
      "Masked lm labels - torch.Size([10, 220])\n",
      "Prediction score shape - torch.Size([10, 220, 1062])\n",
      "[0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 121, 132, 143, 154, 165, 176, 187, 198, 209] for Card\n",
      "Prediction score for loss - torch.Size([200, 3])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 122, 133, 144, 155, 166, 177, 188, 199, 210] for Timestamp\n",
      "Prediction score for loss - torch.Size([200, 10])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[2, 13, 24, 35, 46, 57, 68, 79, 90, 101, 112, 123, 134, 145, 156, 167, 178, 189, 200, 211] for Amount\n",
      "Prediction score for loss - torch.Size([200, 10])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[3, 14, 25, 36, 47, 58, 69, 80, 91, 102, 113, 124, 135, 146, 157, 168, 179, 190, 201, 212] for Use Chip\n",
      "Prediction score for loss - torch.Size([200, 3])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[4, 15, 26, 37, 48, 59, 70, 81, 92, 103, 114, 125, 136, 147, 158, 169, 180, 191, 202, 213] for Merchant Name\n",
      "Prediction score for loss - torch.Size([200, 407])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[5, 16, 27, 38, 49, 60, 71, 82, 93, 104, 115, 126, 137, 148, 159, 170, 181, 192, 203, 214] for Merchant City\n",
      "Prediction score for loss - torch.Size([200, 209])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[6, 17, 28, 39, 50, 61, 72, 83, 94, 105, 116, 127, 138, 149, 160, 171, 182, 193, 204, 215] for Merchant State\n",
      "Prediction score for loss - torch.Size([200, 38])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[7, 18, 29, 40, 51, 62, 73, 84, 95, 106, 117, 128, 139, 150, 161, 172, 183, 194, 205, 216] for Zip\n",
      "Prediction score for loss - torch.Size([200, 284])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[8, 19, 30, 41, 52, 63, 74, 85, 96, 107, 118, 129, 140, 151, 162, 173, 184, 195, 206, 217] for MCC\n",
      "Prediction score for loss - torch.Size([200, 81])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[9, 20, 31, 42, 53, 64, 75, 86, 97, 108, 119, 130, 141, 152, 163, 174, 185, 196, 207, 218] for Errors?\n",
      "Prediction score for loss - torch.Size([200, 8])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "[10, 21, 32, 43, 54, 65, 76, 87, 98, 109, 120, 131, 142, 153, 164, 175, 186, 197, 208, 219] for SPECIAL\n",
      "Prediction score for loss - torch.Size([200, 7])\n",
      "Masked score for loss - torch.Size([200])\n",
      "Masked loss - torch.Size([])\n",
      "Class out - torch.Size([1])\n",
      "tensor([0.5435], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for inps in train_dataloader:\n",
    "    input_ids = inps.pop('input_ids')\n",
    "    print(input_ids.shape)\n",
    "    print(inps['masked_lm_labels'].shape)\n",
    "    #print(inps['masked_lm_labels'], )\n",
    "    print(inps.keys())\n",
    "    \n",
    "    class_out =classif(input_ids, inps)\n",
    "    \n",
    "    print(f\"Class out - {class_out.shape}\")\n",
    "    print(class_out)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. append classifier with bert\n",
    "2. Freeze bert model after first train\n",
    "3. Use bert model and train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a3d88c904243d2c3f246166597f86d1c0a39f3d97496d1fe394945d0c6d436d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
